{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading Training Data...\n",
      "Loading Character 0...\n",
      "Loading Character 1...\n",
      "Loading Character 2...\n",
      "Loading Character 3...\n",
      "Loading Character 4...\n",
      "Loading Character 5...\n",
      "Loading Character 6...\n",
      "Loading Character 7...\n",
      "Loading Character 8...\n",
      "Loading Character 9...\n",
      "Loading Character t...\n",
      "Loading Character d...\n",
      "Loading Character +...\n",
      "Loading Character -...\n",
      "Loading Character =...\n",
      "=> Loading Test Data...\n",
      "Loading Character 0...\n",
      "Loading Character 1...\n",
      "Loading Character 2...\n",
      "Loading Character 3...\n",
      "Loading Character 4...\n",
      "Loading Character 5...\n",
      "Loading Character 6...\n",
      "Loading Character 7...\n",
      "Loading Character 8...\n",
      "Loading Character 9...\n",
      "Loading Character t...\n",
      "Loading Character d...\n",
      "Loading Character +...\n",
      "Loading Character -...\n",
      "Loading Character =...\n",
      "=> Loading Validation Data...\n",
      "Loading Character 0...\n",
      "Loading Character 1...\n",
      "Loading Character 2...\n",
      "Loading Character 3...\n",
      "Loading Character 4...\n",
      "Loading Character 5...\n",
      "Loading Character 6...\n",
      "Loading Character 7...\n",
      "Loading Character 8...\n",
      "Loading Character 9...\n",
      "Loading Character t...\n",
      "Loading Character d...\n",
      "Loading Character +...\n",
      "Loading Character -...\n",
      "Loading Character =...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "# Import Training Data\n",
    "print(\"=> Loading Training Data...\")\n",
    "\n",
    "characters = ['0','1','2','3','4','5','6','7','8','9','t','d','+','-','=']\n",
    "#labels =     [0 , 1,  2,  3,  4,  5,  6,  7,  8,  9,  10,  11, 12, 13, 14]\n",
    "\n",
    "x_train = np.zeros((150542, 45, 45)) #150542 45x45  grayscale images\n",
    "y_train = np.zeros(150542)\n",
    "index = 0\n",
    "\n",
    "for char in characters:\n",
    "    #read each image in the folder for that character\n",
    "    print(\"Loading Character {}...\".format(char))\n",
    "    \n",
    "    #REPLACE WITH YOUR PATH NAME\n",
    "    for img in glob.glob(\"C:\\\\Users\\\\kenne\\\\Downloads\\\\data\\\\extracted_images_train\\\\{}\\\\*.jpg\".format(char)):\n",
    "        n= cv2.imread(img,0)\n",
    "        #add the image to x_train, add the proper label in y_train\n",
    "        x_train[index] = n\n",
    "        y_train[index] = characters.index(char) #char to int\n",
    "        index = index+1\n",
    "\n",
    "        \n",
    "\n",
    "#Import Test Data\n",
    "print(\"=> Loading Test Data...\")\n",
    "        \n",
    "x_test = np.zeros((11412, 45, 45)) #11411 45x45  grayscale images\n",
    "y_test = np.zeros(11412)\n",
    "index = 0\n",
    "\n",
    "for char in characters:\n",
    "    #read each image in the folder for that character\n",
    "    print(\"Loading Character {}...\".format(char))\n",
    "    for img in glob.glob(\"C:\\\\Users\\\\kenne\\\\Downloads\\\\data\\\\extracted_images_test\\\\{}\\\\*.jpg\".format(char)):\n",
    "        n= cv2.imread(img,0)\n",
    "        x_test[index] = n\n",
    "        y_test[index] = characters.index(char) #char to int\n",
    "        index = index+1\n",
    "        \n",
    "        \n",
    "        \n",
    "#Import Validation Data\n",
    "print(\"=> Loading Validation Data...\")\n",
    "        \n",
    "x_validation = np.zeros((8635, 45, 45)) #171,000 45x45  grayscale images\n",
    "y_validation = np.zeros(8635)\n",
    "index = 0\n",
    "\n",
    "for char in characters:\n",
    "    #read each image in the folder for that character\n",
    "    print(\"Loading Character {}...\".format(char))\n",
    "    for img in glob.glob(\"C:\\\\Users\\\\kenne\\\\Downloads\\\\data\\\\extracted_images_validation\\\\{}\\\\*.jpg\".format(char)):\n",
    "        n= cv2.imread(img,0)\n",
    "        x_validation[index] = n\n",
    "        y_validation[index] = characters.index(char) #char to int\n",
    "        index = index+1\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Preprocessing...\n",
      "14.0\n",
      "14.0\n",
      "14.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Dataset data....\n",
      "    x_train shape: (150542, 45, 45, 1)\n",
      "    Number of images in x_train 150542\n",
      "    Number of images in x_test 11412\n",
      "    Number of images in x_val 8635\n",
      "=> Building Model...\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 43, 43, 45)        450       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 21, 21, 45)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 19845)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               2540288   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 2,542,673\n",
      "Trainable params: 2,542,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "=> Training model...\n",
      "Epoch 1/10\n",
      "753/753 [==============================] - 129s 171ms/step - loss: 8989.1650 - sparse_categorical_accuracy: 0.3996 - val_loss: 4.7344 - val_sparse_categorical_accuracy: 0.2556\n",
      "Epoch 2/10\n",
      "753/753 [==============================] - 129s 171ms/step - loss: 9.2323 - sparse_categorical_accuracy: 0.3047 - val_loss: 3.1615 - val_sparse_categorical_accuracy: 0.2764\n",
      "Epoch 3/10\n",
      "753/753 [==============================] - 132s 176ms/step - loss: 2.3025 - sparse_categorical_accuracy: 0.2974 - val_loss: 2.7465 - val_sparse_categorical_accuracy: 0.2726\n",
      "Epoch 4/10\n",
      "753/753 [==============================] - 126s 168ms/step - loss: 5.2342 - sparse_categorical_accuracy: 0.2853 - val_loss: 2.2676 - val_sparse_categorical_accuracy: 0.2555\n",
      "Epoch 5/10\n",
      "753/753 [==============================] - 128s 170ms/step - loss: 2.5602 - sparse_categorical_accuracy: 0.2470 - val_loss: 2.3356 - val_sparse_categorical_accuracy: 0.1778\n",
      "Epoch 6/10\n",
      "753/753 [==============================] - 136s 180ms/step - loss: 2.4329 - sparse_categorical_accuracy: 0.2439 - val_loss: 2.3429 - val_sparse_categorical_accuracy: 0.1715\n",
      "Epoch 7/10\n",
      "753/753 [==============================] - 147s 195ms/step - loss: 6.3380 - sparse_categorical_accuracy: 0.2374 - val_loss: 2.3538 - val_sparse_categorical_accuracy: 0.1704\n",
      "Epoch 8/10\n",
      "753/753 [==============================] - 153s 203ms/step - loss: 2.1985 - sparse_categorical_accuracy: 0.2377 - val_loss: 2.3506 - val_sparse_categorical_accuracy: 0.1701\n",
      "Epoch 9/10\n",
      "753/753 [==============================] - 141s 187ms/step - loss: 2.2475 - sparse_categorical_accuracy: 0.2375 - val_loss: 2.3911 - val_sparse_categorical_accuracy: 0.1702\n",
      "Epoch 10/10\n",
      "753/753 [==============================] - 123s 164ms/step - loss: 2.2032 - sparse_categorical_accuracy: 0.2366 - val_loss: 2.3958 - val_sparse_categorical_accuracy: 0.1684\n",
      "=> Evaluate model...\n",
      "357/357 [==============================] - 4s 12ms/step - loss: 2.2544 - sparse_categorical_accuracy: 0.2886\n",
      "=> Saving model...\n",
      "WARNING:tensorflow:From c:\\users\\kenne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\kenne\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model_data/model_ver_1\\assets\n"
     ]
    }
   ],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"=> Preprocessing...\")\n",
    "\n",
    "# Reshaping the training array to work with TF Keras\n",
    "x_train = x_train.reshape(len(x_train), 45, 45, 1)\n",
    "x_test = x_test.reshape(len(x_test), 45, 45, 1)\n",
    "x_validation = x_validation.reshape(len(x_validation), 45, 45, 1)\n",
    "input_shape = (45, 45, 1)\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_validation = x_validation.astype('float32')\n",
    "\n",
    "# Normalization\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_validation /= 255\n",
    "\n",
    "\n",
    "print(\"=> Dataset data....\")\n",
    "print('    x_train shape:', x_train.shape)\n",
    "print('    Number of images in x_train', x_train.shape[0])\n",
    "print('    Number of images in x_test', x_test.shape[0])\n",
    "print('    Number of images in x_val', x_validation.shape[0])\n",
    "\n",
    "# Building the model\n",
    "print('=> Building Model...')\n",
    "model = Sequential()\n",
    "model.add(Conv2D(45, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(15,activation=tf.nn.softmax)) #possible labels, 0 to 14, so 15 labels\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('=> Training model...')\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy', \n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "# Training model\n",
    "# TODO: Use validation set\n",
    "model.fit(x=x_train,y=y_train, batch_size=200, epochs=10, shuffle=True,validation_data=(x_validation, y_validation))\n",
    "\n",
    "print('=> Evaluate model...')\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "print('=> Saving model...')\n",
    "model.save('model_data/model_ver_1')\n",
    "\n",
    "\n",
    "# To Load the model do this:\n",
    "# new_model = tf.keras.models.load_model('model_data/model_ver_0')\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
